From 726a544034c77835703f7df61632541e460f27e8 Mon Sep 17 00:00:00 2001
From: Ray Griffin <raymond.griffin@protonmail.com>
Date: Mon, 16 Jan 2023 16:43:05 +0000
Subject: [PATCH 3/3] vmov

---
 gcc/config/i386/i386.cc |  64 ++++++++----------------
 gcc/config/i386/sse.md  | 108 +++++++++-------------------------------
 2 files changed, 44 insertions(+), 128 deletions(-)

diff --git a/gcc/config/i386/i386.cc b/gcc/config/i386/i386.cc
index de978d190..fad221e83 100644
--- a/gcc/config/i386/i386.cc
+++ b/gcc/config/i386/i386.cc
@@ -5474,17 +5474,15 @@ ix86_get_ssemov (rtx *operands, unsigned size,
 	{
 	case opcode_int:
 	  if (scalar_mode == E_HFmode || scalar_mode == E_BFmode)
-	    opcode = (misaligned_p
-		      ? (TARGET_AVX512BW ? "vmovdqu16" : "vmovdqu64")
-		      : "vmovdqa64");
+	    opcode = TARGET_AVX512BW ? "vmovdqu16" : "vmovdqu64";
 	  else
-	    opcode = misaligned_p ? "vmovdqu32" : "vmovdqa32";
+	    opcode = "vmovdqu32";
 	  break;
 	case opcode_float:
-	  opcode = misaligned_p ? "vmovups" : "vmovaps";
+	  opcode = "vmovups";
 	  break;
 	case opcode_double:
-	  opcode = misaligned_p ? "vmovupd" : "vmovapd";
+	  opcode = "vmovupd";
 	  break;
 	}
     }
@@ -5495,29 +5493,25 @@ ix86_get_ssemov (rtx *operands, unsigned size,
 	case E_HFmode:
 	case E_BFmode:
 	  if (evex_reg_p)
-	    opcode = (misaligned_p
-		      ? (TARGET_AVX512BW
+	    opcode = TARGET_AVX512BW
 			 ? "vmovdqu16"
-			 : "vmovdqu64")
-		      : "vmovdqa64");
+			 : "vmovdqu64";
 	  else
-	    opcode = (misaligned_p
-		      ? (TARGET_AVX512BW
+	    opcode = TARGET_AVX512BW
 			 ? "vmovdqu16"
-			 : "%vmovdqu")
-		      : "%vmovdqa");
+			 : "%vmovdqu";
 	  break;
 	case E_SFmode:
-	  opcode = misaligned_p ? "%vmovups" : "%vmovaps";
+	  opcode = "%vmovups";
 	  break;
 	case E_DFmode:
-	  opcode = misaligned_p ? "%vmovupd" : "%vmovapd";
+	  opcode = "%vmovupd";
 	  break;
 	case E_TFmode:
 	  if (evex_reg_p)
-	    opcode = misaligned_p ? "vmovdqu64" : "vmovdqa64";
+	    opcode = "vmovdqu64";
 	  else
-	    opcode = misaligned_p ? "%vmovdqu" : "%vmovdqa";
+	    opcode = "%vmovdqu";
 	  break;
 	default:
 	  gcc_unreachable ();
@@ -5529,48 +5523,32 @@ ix86_get_ssemov (rtx *operands, unsigned size,
 	{
 	case E_QImode:
 	  if (evex_reg_p)
-	    opcode = (misaligned_p
-		      ? (TARGET_AVX512BW
-			 ? "vmovdqu8"
-			 : "vmovdqu64")
-		      : "vmovdqa64");
+	    opcode = TARGET_AVX512BW ? "vmovdqu8" : "vmovdqu64";
 	  else
-	    opcode = (misaligned_p
-		      ? (TARGET_AVX512BW
-			 ? "vmovdqu8"
-			 : "%vmovdqu")
-		      : "%vmovdqa");
+	    opcode = TARGET_AVX512BW ? "vmovdqu8" : "%vmovdqu";
 	  break;
 	case E_HImode:
 	  if (evex_reg_p)
-	    opcode = (misaligned_p
-		      ? (TARGET_AVX512BW
-			 ? "vmovdqu16"
-			 : "vmovdqu64")
-		      : "vmovdqa64");
+	    opcode = TARGET_AVX512BW ? "vmovdqu16" : "vmovdqu64";
 	  else
-	    opcode = (misaligned_p
-		      ? (TARGET_AVX512BW
-			 ? "vmovdqu16"
-			 : "%vmovdqu")
-		      : "%vmovdqa");
+	    opcode = TARGET_AVX512BW ? "vmovdqu16" : "%vmovdqu";
 	  break;
 	case E_SImode:
 	  if (evex_reg_p)
-	    opcode = misaligned_p ? "vmovdqu32" : "vmovdqa32";
+	    opcode = "vmovdqu32";
 	  else
-	    opcode = misaligned_p ? "%vmovdqu" : "%vmovdqa";
+	    opcode = "%vmovdqu";
 	  break;
 	case E_DImode:
 	case E_TImode:
 	case E_OImode:
 	  if (evex_reg_p)
-	    opcode = misaligned_p ? "vmovdqu64" : "vmovdqa64";
+	    opcode = "vmovdqu64";
 	  else
-	    opcode = misaligned_p ? "%vmovdqu" : "%vmovdqa";
+	    opcode = "%vmovdqu";
 	  break;
 	case E_XImode:
-	  opcode = misaligned_p ? "vmovdqu64" : "vmovdqa64";
+	  opcode = "vmovdqu64";
 	  break;
 	default:
 	  gcc_unreachable ();
diff --git a/gcc/config/i386/sse.md b/gcc/config/i386/sse.md
index d50627a7d..757ee5912 100644
--- a/gcc/config/i386/sse.md
+++ b/gcc/config/i386/sse.md
@@ -1419,17 +1419,11 @@
 {
   if (FLOAT_MODE_P (GET_MODE_INNER (<MODE>mode)))
     {
-      if (misaligned_operand (operands[1], <MODE>mode))
-	return "vmovu<ssemodesuffix>\t{%1, %0%{%3%}%N2|%0%{%3%}%N2, %1}";
-      else
-	return "vmova<ssemodesuffix>\t{%1, %0%{%3%}%N2|%0%{%3%}%N2, %1}";
+      return "vmovu<ssemodesuffix>\t{%1, %0%{%3%}%N2|%0%{%3%}%N2, %1}";
     }
   else
     {
-      if (misaligned_operand (operands[1], <MODE>mode))
-	return "vmovdqu<ssescalarsize>\t{%1, %0%{%3%}%N2|%0%{%3%}%N2, %1}";
-      else
-	return "vmovdqa<ssescalarsize>\t{%1, %0%{%3%}%N2|%0%{%3%}%N2, %1}";
+      return "vmovdqu<ssescalarsize>\t{%1, %0%{%3%}%N2|%0%{%3%}%N2, %1}";
     }
 }
   [(set_attr "type" "ssemov")
@@ -1620,17 +1614,11 @@
 {
   if (FLOAT_MODE_P (GET_MODE_INNER (<MODE>mode)))
     {
-      if (misaligned_operand (operands[0], <MODE>mode))
-	return "vmovu<ssemodesuffix>\t{%1, %0%{%2%}|%0%{%2%}, %1}";
-      else
-	return "vmova<ssemodesuffix>\t{%1, %0%{%2%}|%0%{%2%}, %1}";
+      return "vmovu<ssemodesuffix>\t{%1, %0%{%2%}|%0%{%2%}, %1}";
     }
   else
     {
-      if (misaligned_operand (operands[0], <MODE>mode))
-	return "vmovdqu<ssescalarsize>\t{%1, %0%{%2%}|%0%{%2%}, %1}";
-      else
-	return "vmovdqa<ssescalarsize>\t{%1, %0%{%2%}|%0%{%2%}, %1}";
+      return "vmovdqu<ssescalarsize>\t{%1, %0%{%2%}|%0%{%2%}, %1}";
     }
 }
   [(set_attr "type" "ssemov")
@@ -10449,7 +10437,7 @@
   "TARGET_SSE && !(MEM_P (operands[0]) && MEM_P (operands[1]))"
   "@
    %vmovlps\t{%1, %0|%q0, %1}
-   %vmovaps\t{%1, %0|%0, %1}
+   %vmovups\t{%1, %0|%0, %1}
    %vmovlps\t{%1, %d0|%d0, %q1}"
   [(set_attr "type" "ssemov")
    (set_attr "prefix" "maybe_vex")
@@ -18257,29 +18245,15 @@
   switch (<MODE>mode)
     {
     case E_V8DFmode:
-      if (misaligned_operand (operands[2], <ssequartermode>mode))
-	return "vmovupd\t{%2, %x0|%x0, %2}";
-      else
-	return "vmovapd\t{%2, %x0|%x0, %2}";
+      return "vmovupd\t{%2, %x0|%x0, %2}";
     case E_V16SFmode:
-      if (misaligned_operand (operands[2], <ssequartermode>mode))
-	return "vmovups\t{%2, %x0|%x0, %2}";
-      else
-	return "vmovaps\t{%2, %x0|%x0, %2}";
+      return "vmovups\t{%2, %x0|%x0, %2}";
     case E_V8DImode:
-      if (misaligned_operand (operands[2], <ssequartermode>mode))
-	return which_alternative == 2 ? "vmovdqu64\t{%2, %x0|%x0, %2}"
+      return which_alternative == 2 ? "vmovdqu64\t{%2, %x0|%x0, %2}"
 				      : "vmovdqu\t{%2, %x0|%x0, %2}";
-      else
-	return which_alternative == 2 ? "vmovdqa64\t{%2, %x0|%x0, %2}"
-				      : "vmovdqa\t{%2, %x0|%x0, %2}";
     case E_V16SImode:
-      if (misaligned_operand (operands[2], <ssequartermode>mode))
-	return which_alternative == 2 ? "vmovdqu32\t{%2, %x0|%x0, %2}"
+      return which_alternative == 2 ? "vmovdqu32\t{%2, %x0|%x0, %2}"
 				      : "vmovdqu\t{%2, %x0|%x0, %2}";
-      else
-	return which_alternative == 2 ? "vmovdqa32\t{%2, %x0|%x0, %2}"
-				      : "vmovdqa\t{%2, %x0|%x0, %2}";
     default:
       gcc_unreachable ();
     }
@@ -26765,63 +26739,27 @@
       switch (get_attr_mode (insn))
 	{
 	case MODE_V16SF:
-	  if (misaligned_operand (operands[1], <ssehalfvecmode>mode))
-	    return "vmovups\t{%1, %t0|%t0, %1}";
-	  else
-	    return "vmovaps\t{%1, %t0|%t0, %1}";
+	  return "vmovups\t{%1, %t0|%t0, %1}";
 	case MODE_V8DF:
-	  if (misaligned_operand (operands[1], <ssehalfvecmode>mode))
-	    return "vmovupd\t{%1, %t0|%t0, %1}";
-	  else
-	    return "vmovapd\t{%1, %t0|%t0, %1}";
+	  return "vmovupd\t{%1, %t0|%t0, %1}";
 	case MODE_V8SF:
-	  if (misaligned_operand (operands[1], <ssehalfvecmode>mode))
-	    return "vmovups\t{%1, %x0|%x0, %1}";
-	  else
-	    return "vmovaps\t{%1, %x0|%x0, %1}";
+	  return "vmovups\t{%1, %x0|%x0, %1}";
 	case MODE_V4DF:
-	  if (misaligned_operand (operands[1], <ssehalfvecmode>mode))
-	    return "vmovupd\t{%1, %x0|%x0, %1}";
-	  else
-	    return "vmovapd\t{%1, %x0|%x0, %1}";
+	  return "vmovupd\t{%1, %x0|%x0, %1}";
 	case MODE_XI:
-	  if (misaligned_operand (operands[1], <ssehalfvecmode>mode))
-	    {
-	      if (which_alternative == 2)
-		return "vmovdqu\t{%1, %t0|%t0, %1}";
-	      else if (GET_MODE_SIZE (<ssescalarmode>mode) == 8)
-		return "vmovdqu64\t{%1, %t0|%t0, %1}";
-	      else
-		return "vmovdqu32\t{%1, %t0|%t0, %1}";
-	    }
+	  if (which_alternative == 2)
+	    return "vmovdqu\t{%1, %t0|%t0, %1}";
+	  else if (GET_MODE_SIZE (<ssescalarmode>mode) == 8)
+	    return "vmovdqu64\t{%1, %t0|%t0, %1}";
 	  else
-	    {
-	      if (which_alternative == 2)
-		return "vmovdqa\t{%1, %t0|%t0, %1}";
-	      else if (GET_MODE_SIZE (<ssescalarmode>mode) == 8)
-		return "vmovdqa64\t{%1, %t0|%t0, %1}";
-	      else
-		return "vmovdqa32\t{%1, %t0|%t0, %1}";
-	    }
+	    return "vmovdqu32\t{%1, %t0|%t0, %1}";
 	case MODE_OI:
-	  if (misaligned_operand (operands[1], <ssehalfvecmode>mode))
-	    {
-	      if (which_alternative == 2)
-		return "vmovdqu\t{%1, %x0|%x0, %1}";
-	      else if (GET_MODE_SIZE (<ssescalarmode>mode) == 8)
-		return "vmovdqu64\t{%1, %x0|%x0, %1}";
-	      else
-		return "vmovdqu32\t{%1, %x0|%x0, %1}";
-	    }
+	  if (which_alternative == 2)
+	    return "vmovdqu\t{%1, %x0|%x0, %1}";
+	  else if (GET_MODE_SIZE (<ssescalarmode>mode) == 8)
+	    return "vmovdqu64\t{%1, %x0|%x0, %1}";
 	  else
-	    {
-	      if (which_alternative == 2)
-		return "vmovdqa\t{%1, %x0|%x0, %1}";
-	      else if (GET_MODE_SIZE (<ssescalarmode>mode) == 8)
-		return "vmovdqa64\t{%1, %x0|%x0, %1}";
-	      else
-		return "vmovdqa32\t{%1, %x0|%x0, %1}";
-	    }
+	    return "vmovdqu32\t{%1, %x0|%x0, %1}";
 	default:
 	  gcc_unreachable ();
 	}
-- 
2.39.0

